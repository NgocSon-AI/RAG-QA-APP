import os
import streamlit as st
from dotenv import load_dotenv
load_dotenv()
#from logger import logger
from rag_utils import read_file, split_text, get_embedding_model, build_vectorstore
import time
# GROQ SDK
from groq import Groq

from sklearn.metrics.pairwise import cosine_similarity


st.sidebar.title("C·∫•u h√¨nh m√¥ h√¨nh")

CHAT_MODELS = {
    "Llama 3.1 8B Instant": {"type": "groq", "model_name": "llama-3.1-8b-instant"},
    "meta-llama/llama-guard-4-12b": {"type": "groq", "model_name": "meta-llama/llama-guard-4-12b"},
    "gemma2-9b-it": {"type": "groq", "model_name": "gemma2-9b-it"},
    "moonshotai/kimi-k2-instruct-0905": {"type": "groq", "model_name": "moonshotai/kimi-k2-instruct-0905"},
    "openai/gpt-oss-20b": {"type": "groq", "model_name": "openai/gpt-oss-20b"},
    "groq/compound": {"type": "groq", "model_name": "groq/compound"},
}

selected_chat_model = st.sidebar.selectbox("Ch·ªçn m√¥ h√¨nh chat:", list(CHAT_MODELS.keys()))

EMBEDDING_MODELS = {
    "(offline) multilingual-e5-small": {"type": "sentence-transformer", "model_name": "intfloat/multilingual-e5-small"},
    "(offline) multilingual-MiniLM-L12-v2": {"type": "sentence-transformer", "model_name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"},
    "(offline) distiluse-base-multilingual-cased-v1": {"type": "sentence-transformer", "model_name": "sentence-transformers/distiluse-base-multilingual-cased-v1"},
    "(offline) multilingual-e5-base": {"type": "sentence-transformer", "model_name": "intfloat/multilingual-e5-base"}, 
    "OpenAIEmbeddings": {"type": "openai"},
}
selected_embedding_model = st.sidebar.selectbox("Ch·ªçn m√¥ h√¨nh embedding:", list(EMBEDDING_MODELS.keys()))

USE_FAISS = True

#from logger import logger

uploaded_files = st.file_uploader(
    "Upload nhi·ªÅu file c√πng l√∫c", 
    type=["pdf","docx","txt","csv"], 
    accept_multiple_files=True
)

if uploaded_files:
    #logger.info({"event": "file_upload", "num_files": len(uploaded_files),
    #             "files": [f.name for f in uploaded_files]})
    
    all_texts = []
    for file in uploaded_files:
        text = read_file(file)
        if text:
            chunks = split_text(text)
            if chunks:
                all_texts.extend(chunks)
    st.success(f"ƒê√£ chia vƒÉn b·∫£n th√†nh {len(all_texts)} ƒëo·∫°n!")
    #logger.info({"event": "text_split", "num_chunks": len(all_texts)})


    embed_info = EMBEDDING_MODELS[selected_embedding_model]
    if embed_info["type"] == "openai":
        embedding_model = get_embedding_model(openai_api_key=os.getenv("OPENAI_API_KEY"))
    else:
        embedding_model = get_embedding_model(model_name=embed_info["model_name"])

    vectordb = build_vectorstore(all_texts, embedding_model, use_faiss=USE_FAISS)
    st.session_state['vectordb'] = vectordb
    st.success("‚úÖ Vector store ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!")


# ---------------- QA ----------------
if 'vectordb' in st.session_state:
    query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n b·∫±ng ti·∫øng Vi·ªát:")

    if query:
        # --- Initialize GROQ client ---
        GROQ_API_KEY = os.getenv("GROQ_API_KEY")
        client = Groq(api_key=GROQ_API_KEY)

        #logger.info({"event": "query_received", "query": query})

        # --- Top-k retrieval ---
        
        docs_and_scores = st.session_state['vectordb'].similarity_search_with_score(query, k=5)
        # Gh√©p context t·ª´ vƒÉn b·∫£n
        context_text = "\n\n".join([doc.page_content for doc, score in docs_and_scores])

        # retriever = st.session_state['vectordb'].as_retriever(search_kwargs={"k":5})
        
        #relevant_chunks = docs_and_scores.get_relevant_documents(query)
        
        #logger.info({"event": "retrieval_done", "num_docs": len(docs_and_scores)})
        
        #context_text = "\n\n".join([doc.page_content for doc in relevant_chunks])

        # --- Prompt ---
        prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI am hi·ªÉu s√¢u r·ªông v·ªÅ tr√≠ tu·ªá nh√¢n t·∫°o. 
Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p. 
N·∫øu t√†i li·ªáu kh√¥ng c√≥ th√¥ng tin li√™n quan, h√£y tr·∫£ l·ªùi ƒë√∫ng nguy√™n vƒÉn:
"t√¥i kh√¥ng bi·∫øt, ki·∫øn th·ª©c v·ªÅ c√¢u h·ªèi kh√¥ng c√≥ trong t√†i li·ªáu".

Y√™u c·∫ßu khi tr·∫£ l·ªùi:
- Ng·∫Øn g·ªçn, s√∫c t√≠ch.
- Tr√¨nh b√†y theo g·∫°ch ƒë·∫ßu d√≤ng t·ª´ng √Ω.
- Ch·ªâ d·ª±a tr√™n th√¥ng tin c√≥ trong t√†i li·ªáu.

V√≠ d·ª• minh h·ªça:
"Supervised learning (H·ªçc c√≥ gi√°m s√°t)" l√†:
- M·ªôt k·ªπ thu·∫≠t h·ªçc m√°y.
- Thu·∫≠t to√°n ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n d·ªØ li·ªáu ƒë√£ g√°n nh√£n (ƒë·∫ßu v√†o v√† ƒë·∫ßu ra).
- Gi√∫p d·ª± ƒëo√°n k·∫øt qu·∫£ cho d·ªØ li·ªáu m·ªõi.

"CNN" l√†:
- Convolutional Neural Network (M·∫°ng N∆°-ron T√≠ch ch·∫≠p).
- M√¥ h√¨nh h·ªçc s√¢u cho x·ª≠ l√Ω ·∫£nh v√† th·ªã gi√°c m√°y t√≠nh.
- D√πng c√°c t·∫ßng t√≠ch ch·∫≠p ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng.

‚ÄúPrompt‚Äù l√† c√°ch b·∫°n giao ti·∫øp v·ªõi AI, v√† vi·ªác t·∫°o ra m·ªôt prompt hi·ªáu qu·∫£ ƒë√≥ng vai tr√≤ quan tr·ªçng trong vi·ªác ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ mong mu·ªën. M·ªôt c·∫•u tr√∫c chu·∫©n ƒë·ªÉ t·∫°o prompt:
- Ng·ªØ c·∫£nh: M√¥ t·∫£ ch·ªß ƒë·ªÅ ho·∫∑c v·∫•n ƒë·ªÅ b·∫°n c·∫ßn AI h·ªó tr·ª£.
- H∆∞·ªõng d·∫´n: ƒê∆∞a ra y√™u c·∫ßu c·ª• th·ªÉ.
- D·ªØ li·ªáu ƒë·∫ßu v√†o: Cung c·∫•p th√¥ng tin ho·∫∑c d·ªØ li·ªáu cho AI s·ª≠ d·ª•ng.
- K·∫øt qu·∫£ mong mu·ªën: Ch·ªâ ƒë·ªãnh r√µ k·∫øt qu·∫£ b·∫°n k·ª≥ v·ªçng.
---

VƒÉn b·∫£n tham kh·∫£o:
{context_text}

C√¢u h·ªèi:
{query}
        """

        # --- G·ªçi GROQ API ---
        response = client.chat.completions.create(
            model=CHAT_MODELS[selected_chat_model]["model_name"],
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_completion_tokens=300
        )

        answer = response.choices[0].message.content
        st.markdown(f"**Tr·∫£ l·ªùi:** {answer}")
        #logger.info({"event": "response_sent", "answer": answer})

        st.markdown("### üîπ T√†i li·ªáu tham kh·∫£o")
        for i, (doc, score) in enumerate(docs_and_scores, 1):
            source = doc.metadata.get("source", "Unknown")  # l·∫•y metadata ngu·ªìn
            with st.expander(f"üîπ Chunk {i} (Ngu·ªìn: {source}, ƒê·ªô t∆∞∆°ng ƒë·ªìng: {score:.4f})"):
                st.write(doc.page_content)
